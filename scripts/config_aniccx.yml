general:
  me: config.yml                                # path to this file
  device: [cuda:0,cuda:1,cuda:2,cuda:3]      # cpu / cuda:0 / list of cuda
  driver: train.py   # path to the run script
  output: [local/ccsd/test , 1]                 # path and iterator for the output directory

data:
  train_path: /shared/jacklishufan/ani-1-cxx
  test_path: /shared/jacklishufan/rmd17/npz_data
  test_method: "all" # other options are 'out' and 'in', corresponding to in-distribution and out-distribution
  test_size_per_molecule: 0.25
  subtract_self_energy: on 
  cutoff: 5.0                 # cutoff radius
  train_size_proportion: 0.1             # training set size: to sample randomly
  val_size_proportion: 0.01                # validation set size: to sample randomly
  test_size: 1000             # test set size: to sample randomly
  random_states: 90           # random seed for data splitting
  train_val_split_random_state: 0
  train_test_split_random_state: 0

model:
  pre_trained: False          # path to the previously trained model for warm-up start
  activation: swish           # activation function: swish, ssp, relu, ...
  requires_dr: True           # if derivative of the output is required
  w_energy: 1.0              # the weight of energy loss in the loss function
  w_force: 0.0           # the weight of force loss in the loss function
  wf_decay: 0.0               # rate of exponential decay of force wight by training epoch
  w_f_mag: 0.0                # the weight of force magnitude loss in the loss function
  w_f_dir: 0.0                # the weight of force direction loss in the loss function
  resolution: 20              # number of basis functions that describe interatomic distances
  n_features: 128             # number of features
  max_z: 26                   # maximum atomic number in the chemical systems
  n_interactions: 3           # number of interaction blocks of newtonnet
  cutoff_network: poly        # the cutoff function: poly (polynomial), cosine
  normalize_atomic: on     # if True the atomic energy needs to be inverse normalized, otherwise total energy will be scaled back
  shared_interactions: False  # if True parameters of interaction blocks will be shared.
  normalize_filter: on     #
  return_latent: off         # if True, the latent space will be returned for the future investigation
  double_update_latent: False #
  nonlinear_attention: linear # if on, add non linear attention layer, if set to string 'linear', then reduce back to linear attention
  attention_heads: 32 # number of heads in multihead attantion
  three_body: off # whether to use 3-Body interaction. This only works when nonlinear_attention is on (and not 'linear').
  layer_norm: true           # normalize hidden layer with a 1D layer_norm function
  aggregration: sum           # aggregration function to use if the prediction may not be represented as the sum of atomic components. 
                              # Options are: sum, mean, max

training:
  epochs: 100                # number of times the entire training data will be shown to the model
  tr_batch_size: 64           # number of training points (snapshots) in a batch of data that is feed to the model
  val_batch_size: 64          # number of validation points (snapshots) in a batch of data that is feed to the model
  tr_rotations: 0             # number of times the training data needs to be randomly rotated (redundant for NewtonNet model)
  val_rotations: 0            # number of times the validation data needs to be randomly rotated (redundant for NewtonNet model)
  tr_frz_rot: False           # if True, fixed rotations matrix will be used at each epoch
  val_frz_rot: False          #
  tr_keep_original: True      # if True, the original orientation of data will be preserved as part of training set (beside other rotations)
  val_keep_original: True     #
  shuffle: True               # shuffle training data before each epoch
  drop_last: True             # if True, drop the left over data points that are less than a full batch size
  lr: 1.0e-4               # learning rate
  lr_scheduler: [plateau, 15, 30, 0.7, 1.0e-6]    # the learning rate decay based on the plateau algorithm: n_epoch_averaging, patience, decay_rate, stop_lr
#  lr_scheduler: [decay, 0.05]                    # the learning rate decay based on exponential decay: the rate of decay
  weight_decay: 0             # the l2 norm
  dropout: 0              # dropout between 0 and 1

hooks:
  vismolvector3d: False       # if the latent force vectors need to be visualized (only works when the return_latent is on)

checkpoint:
  log: 5                     # log the results every this many epochs
  val: 5                      # evaluate the performance on the validation set every this many epochs
  test: 9999                     # evaluate the performance on the test set every this many epochs
  model: 100                  # save the model every this many epochs
  verbose: False              # verbosity of the logging




